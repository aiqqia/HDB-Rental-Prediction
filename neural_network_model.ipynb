{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data:  (48000, 13)\n",
      "Shape of training label:  (48000, 1)\n",
      "Shape of validation data:  (12000, 13)\n",
      "Shape of validation label:  (12000, 1)\n",
      "Shape of testing data:  (30000, 13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6000 [00:00<?, ?it/s]/opt/homebrew/lib/python3.11/site-packages/torch/autograd/__init__.py:200: UserWarning: The operator 'aten::sgn.out' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:11.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import warnings\n",
    "\n",
    "# Suppress the FutureWarning related to is_categorical_dtype from TargetEncoder\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "class MyTargetEncoder():\n",
    "    def __init__(self, columns_to_target_encode, training_data):\n",
    "        self.encoders = {}\n",
    "        self.columns_to_target_encode = columns_to_target_encode\n",
    "        for col in columns_to_target_encode:\n",
    "            encoder = TargetEncoder()\n",
    "            encoder.fit(training_data[col], training_data['monthly_rent'])\n",
    "            self.encoders[col] = encoder\n",
    "        \n",
    "    def fit_data(self, encoded_data):\n",
    "        for col, encoder in self.encoders.items():\n",
    "            encoded_data[col] = encoder.transform(encoded_data[col])\n",
    "        return encoded_data\n",
    "\n",
    "\n",
    "def clean_data(data):\n",
    "    cleaned_data = data\n",
    "    # cleaned_data = cleaned_data.drop_duplicates(subset=None, keep='first', inplace=False)\n",
    "    cleaned_data = cleaned_data.drop(columns=['furnished', 'elevation', 'town', 'block', 'street_name', 'planning_area'])\n",
    "    cleaned_data['flat_type'] = cleaned_data['flat_type'].str.replace(r'(2|3|4|5)-room|(\\d) room', r'\\1\\2', regex=True)\n",
    "    cleaned_data['flat_type'] = cleaned_data['flat_type'].str.replace('executive', '6')\n",
    "    cleaned_data['flat_type'] = cleaned_data['flat_type'].astype(int)\n",
    "    cleaned_data['rent_approval_date'] = cleaned_data['rent_approval_date'].str[2:].str.replace('-', '', regex=False)\n",
    "    cleaned_data['rent_approval_date'] = cleaned_data['rent_approval_date'].astype(int)\n",
    "    \n",
    "    return cleaned_data\n",
    "\n",
    "\n",
    "def encode_data(train_org, training_cleaned, valid_cleaned, testing_cleaned):\n",
    "    # First Target Encoding\n",
    "    \n",
    "    columns_to_target_encode = ['flat_model', 'subzone']\n",
    "    myTargetEncoder = MyTargetEncoder(columns_to_target_encode, train_org)\n",
    "    \n",
    "    training_encoded = myTargetEncoder.fit_data(training_cleaned)\n",
    "    valid_encoded = myTargetEncoder.fit_data(valid_cleaned)\n",
    "    testing_encoded = myTargetEncoder.fit_data(testing_cleaned)\n",
    "    \n",
    "    # Now, One-Hot Encoding\n",
    "    \n",
    "    # Prepare Model\n",
    "    myOneHotEncoder = OneHotEncoder(sparse=False)\n",
    "    myOneHotEncoder.fit(training_encoded[['region']])\n",
    "    \n",
    "    # Fit on train data\n",
    "    tr1 = myOneHotEncoder.transform(training_encoded[['region']])\n",
    "    tr2 = pd.DataFrame(tr1, columns=myOneHotEncoder.get_feature_names_out(['region']))\n",
    "    tr3 = pd.concat([training_encoded.reset_index(drop=True), tr2.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    training_encoded = tr3.drop(columns=[\"region\"])\n",
    "    \n",
    "    # Fit on valid data\n",
    "    va1 = myOneHotEncoder.transform(valid_encoded[['region']])\n",
    "    va2 = pd.DataFrame(va1, columns=myOneHotEncoder.get_feature_names_out(['region']))\n",
    "    va3 = pd.concat([valid_encoded.reset_index(drop=True), va2.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    valid_encoded = va3.drop(columns=[\"region\"])\n",
    "    \n",
    "    # Fit on test data\n",
    "    te1 = myOneHotEncoder.transform(testing_encoded[['region']])\n",
    "    te2 = pd.DataFrame(te1, columns=myOneHotEncoder.get_feature_names_out(['region']))\n",
    "    te3 = pd.concat([testing_encoded.reset_index(drop=True), te2.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    testing_encoded = te3.drop(columns=[\"region\"])\n",
    "    \n",
    "    return training_encoded, valid_encoded, testing_encoded\n",
    "\n",
    "\n",
    "def scale_data(training_encoded, validation_encoded, testing_encoded):\n",
    "    scaler = StandardScaler()\n",
    "    training_scaled = scaler.fit_transform(training_encoded)\n",
    "    validation_scaled = scaler.fit_transform(validation_encoded)\n",
    "    testing_scaled = scaler.fit_transform(testing_encoded)\n",
    "    return training_scaled, validation_scaled, testing_scaled\n",
    "\n",
    "def preprocess_data(train_org, training_data_raw, valid_data_raw, testing_data_raw):\n",
    "    \n",
    "    training_cleaned = clean_data(training_data_raw)\n",
    "    valid_cleaned = clean_data(valid_data_raw)\n",
    "    testing_cleaned = clean_data(testing_data_raw)\n",
    "    \n",
    "    training_encoded, valid_encoded, testing_encoded = encode_data(train_org, training_cleaned, valid_cleaned, testing_cleaned)\n",
    "\n",
    "    return training_encoded, valid_encoded, testing_encoded\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x_train, y_train):\n",
    "        self.x = torch.tensor(x_train, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        features = self.x[idx]\n",
    "        label = self.y[idx]\n",
    "\n",
    "        return features, label\n",
    "    \n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, test_data):\n",
    "        self.data = test_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        features = torch.FloatTensor(sample)  # Assuming the features are in a list or NumPy array\n",
    "        return features\n",
    "\n",
    "def get_data_loaders(X_train, y_train, X_val, y_val, X_test, batch_size):\n",
    "    train_dataset = CustomDataset(X_train, y_train.to_numpy())\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    valid_dataset = CustomDataset(X_val, y_val.to_numpy())\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    test_dataset = TestDataset(X_test)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, valid_loader, test_loader\n",
    "\n",
    "\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)  # 13 input features, 64 hidden units\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 64)  # 64 hidden units, 32 hidden units\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(64, 32)  # 32 hidden units, 1 output\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(32,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    training_data_raw = pd.read_csv('train.csv')\n",
    "    testing_data_raw = pd.read_csv('test.csv')\n",
    "    \n",
    "    train_X, train_y = training_data_raw.drop('monthly_rent', axis=1), training_data_raw[['monthly_rent']]\n",
    "    X_train, X_val, y_train, y_val = train_test_split(train_X, train_y, test_size=0.2, random_state=42)\n",
    "    X_train, X_val, testing_data = preprocess_data(training_data_raw, X_train, X_val, testing_data_raw)\n",
    "\n",
    "    \n",
    "    X_train, X_val, X_test = scale_data(X_train, X_val, testing_data)\n",
    "    \n",
    "    print(\"Shape of training data: \", X_train.shape)\n",
    "    print(\"Shape of training label: \", y_train.shape)\n",
    "    print(\"Shape of validation data: \", X_val.shape)\n",
    "    print(\"Shape of validation label: \", y_val.shape)\n",
    "    print(\"Shape of testing data: \", X_test.shape)\n",
    "        \n",
    "    train_loader, val_loader, test_loader = get_data_loaders(X_train, y_train, X_val, y_val, X_test, batch_size=8)\n",
    "        \n",
    "    input_size = X_train.shape[1]\n",
    "    \n",
    "    device = \"mps\"\n",
    "    model = RegressionModel(input_size)\n",
    "    model.to(device)\n",
    "    \n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer=optimizer,\n",
    "        step_size=1,\n",
    "        gamma=0.9,\n",
    "        verbose=False,\n",
    "    )\n",
    "    \n",
    "    max_epochs = 50\n",
    "    \n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    \n",
    "    min_loss = float(\"inf\")\n",
    "    \n",
    "    for e in range(max_epochs):\n",
    "        \n",
    "        # TRAINING\n",
    "        model.train()\n",
    "        losses = []\n",
    "        for inp, label in tqdm(train_loader):\n",
    "            inp = inp.to(device)\n",
    "            label = label.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(inp)\n",
    "            loss = criterion(y_pred, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        epoch_train_loss = np.mean(losses)\n",
    "        \n",
    "        # VALIDATING\n",
    "        model.eval()\n",
    "        losses = []\n",
    "        with torch.no_grad():\n",
    "            for inp, label in tqdm(val_loader):\n",
    "                inp = inp.to(device)\n",
    "                label = label.to(device)\n",
    "                y_pred = model(inp)\n",
    "                loss = criterion(y_pred, label)\n",
    "\n",
    "                losses.append(loss.item())\n",
    "\n",
    "        epoch_valid_loss = np.mean(losses)\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        print(\"\\nEpoch {}: Training loss: {} Validation Loss: {}\".format(e+1, epoch_train_loss, epoch_valid_loss))\n",
    "        \n",
    "        if epoch_valid_loss < min_loss:\n",
    "            min_loss = epoch_valid_loss\n",
    "            torch.save(model.state_dict(), \"final_model.pth\")\n",
    "        \n",
    "    \n",
    "    fin_model = RegressionModel(input_size)\n",
    "    fin_model.to(device)\n",
    "    fin_model.load_state_dict(torch.load(\"final_model.pth\"))\n",
    "    \n",
    "    model.eval()\n",
    "    losses = []\n",
    "    final_pred = []\n",
    "    with torch.no_grad():\n",
    "        for inp in tqdm(test_loader):\n",
    "            inp = inp.to(device)\n",
    "            y_pred = model(inp)\n",
    "            final_pred.append(y_pred)\n",
    "\n",
    "    print(\"Length of final predictions is: \", len(final_pred))\n",
    "    combined_tensor = torch.cat(final_pred, dim=0)\n",
    "    numpy_array = combined_tensor.cpu().numpy()\n",
    "    flattened_array = numpy_array.flatten()\n",
    "    ids = np.arange(30000)\n",
    "    df = pd.DataFrame({'Id': ids, 'Predicted': flattened_array})\n",
    "\n",
    "    df.to_csv(\"submission1.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
